
"""
======================
Tabular Classification
======================
"""
import os
import typing

import sklearn.datasets
import sklearn.model_selection

from autoPyTorch.datasets.tabular_dataset import TabularDataset
from autoPyTorch.pipeline.tabular_classification import TabularClassificationPipeline
from autoPyTorch.utils.backend import create
from autoPyTorch.utils.pipeline import get_dataset_requirements


# Get the training data for tabular classification
def get_data_to_train() -> typing.Tuple[typing.Any, typing.Any, typing.Any, typing.Any]:
    """
    This function returns a fit dictionary that within itself, contains all
    the information to fit a pipeline
    """

    # Get the training data for tabular classification
    # Move to Australian to showcase numerical vs categorical
    X, y = sklearn.datasets.fetch_openml(data_id=40981, return_X_y=True, as_frame=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X,
        y,
        random_state=1,
        test_size=0.2,
    )

    return X_train, X_test, y_train, y_test


if __name__ == '__main__':
    # Get data to train
    X_train, X_test, y_train, y_test = get_data_to_train()

    # Build a repository with random fitted models
    backend = create(temporary_directory='./tmp/autoPyTorch_tabular_classification_tmp',
                     output_directory='./tmp/autoPyTorch_tabular_classification_out',
                     delete_tmp_folder_after_terminate=False)
    # Create the directory structure
    backend._make_internals_directory()

    # Create a datamanager for this toy problem
    datamanager = TabularDataset(
        X=X_train, Y=y_train,
        X_test=X_test, Y_test=y_test)
    backend.save_datamanager(datamanager)

    info = {'task_type': datamanager.task_type,
            'output_type': datamanager.output_type,
            'issparse': datamanager.issparse,
            'numerical_columns': datamanager.numerical_columns,
            'categorical_columns': datamanager.categorical_columns}

    dataset_properties = datamanager.get_dataset_properties(get_dataset_requirements(info))
    pipeline = TabularClassificationPipeline(dataset_properties=dataset_properties)
    split_id = 0
    train_indices, val_indices = datamanager.splits[split_id]
    # Create a fit dictionary
    fit_dictionary = {
        'X_train': X_train,
        'y_train': y_train,
        'train_indices': train_indices,
        'val_indices': val_indices,
        'X_test': X_test,
        'y_test': y_test,
        'dataset_properties': dataset_properties,
        # Training configuration
        'job_id': 'example_tabular_classification_1',
        'working_dir': './tmp/example_tabular_classification_1',  # Hopefully generated by backend
        'device': 'cpu',
        'budget_type': 'epochs',
        'epochs': 1,
        'runtime': 3600,
        'torch_num_threads': 1,
        'early_stopping': 20,
        'use_tensorboard_logger': True,
        'use_pynisher': False,
        'metrics_during_training': True,
        'backend': backend,
        'split_id': split_id,
    }

    # Configuration space
    pipeline_cs = pipeline.get_hyperparameter_search_space()
    print("Pipeline CS:\n", '_' * 40, f"\n{pipeline_cs}")
    config = pipeline_cs.sample_configuration()
    print("Pipeline Random Config:\n", '_' * 40, f"\n{config}")
    pipeline.set_hyperparameters(config)

    # Make sure the working directory exists. Something that backend will handle
    os.makedirs('./tmp/example_tabular_classification_1', exist_ok=True)

    # Fit the pipeline
    print("Fitting the pipeline...")
    pipeline.fit(fit_dictionary)

    # Showcase some components of the pipeline
    print(pipeline)

    # Showcase performance of pipeline
    print(pipeline.named_steps['trainer'].run_summary.repr_last_epoch())
